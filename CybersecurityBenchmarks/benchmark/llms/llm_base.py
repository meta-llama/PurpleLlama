# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

# pyre-strict

from __future__ import annotations

import logging
import textwrap
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Callable, List, Optional, Union

from ..cache_handler import CacheHandler

NUM_LLM_RETRIES = 100
MAX_RETRY_TIMEOUT = 600

LOG: logging.Logger = logging.getLogger(__name__)

DEFAULT_MAX_TOKENS = 2048
DEFAULT_TEMPERATURE = 0.6
DEFAULT_TOP_P = 0.9


class RetriesExceededException(Exception):
    def __init__(
        self, message: str = "Maximum number of retries has been exceeded"
    ) -> None:
        self.message = message
        super().__init__(self.message)


class UnretriableQueryException(Exception):
    pass


@dataclass
class LLMConfig:
    model: str
    api_key: str | None = None
    max_tokens: int = DEFAULT_MAX_TOKENS
    cot_end_tokens: list[str] | None = None


class LLM(ABC):
    def __init__(
        self,
        config: LLMConfig,
    ) -> None:
        self.model: str = config.model
        self.api_key: str | None = config.api_key
        self.cache_handler: CacheHandler | None = None
        self.max_tokens: int = config.max_tokens
        self.cot_end_tokens: list[str] | None = config.cot_end_tokens
        if self.model not in self.valid_models():
            LOG.warning(
                f"{self.model} is not in the valid model list for {type(self).__name__}. Valid models are: {', '.join(self.valid_models())}."
            )

    @abstractmethod
    def query(
        self,
        prompt: str,
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        """
        Abstract method to query an LLM with a given prompt and return the response.

        Args:
            prompt (str): The prompt to send to the LLM.
            guided_decode_json_schema (Optional[str]): JSON schema for guided decoding.
            temperature (float): Controls randomness in the output.
            top_p (float): Controls diversity via nucleus sampling.

        Returns:
            str: The response from the LLM.
        """
        pass

    @abstractmethod
    def chat(
        self,
        prompt_with_history: List[str],
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        """
        Abstract method to chat with an LLM with given prompts and return the response.

        Args:
            prompt_with_history ([str]): The prompt to send to the LLM along with chat history. The last string in the list is the prompt.
            guided_decode_json_schema (Optional[str]): JSON schema for guided decoding.
            temperature (float): Controls randomness in the output.
            top_p (float): Controls diversity via nucleus sampling.

        Returns:
            str: The response from the LLM.
        """
        pass

    @abstractmethod
    def chat_with_system_prompt(
        self,
        system_prompt: str,
        prompt_with_history: List[str],
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        """
        Abstract method to chat with an LLM with given prompts as well as system prompt and return the response.

        Args:
            system_prompt (str): The system prompt to send to the LLM.
            prompt_with_history ([str]): The prompt to send to the LLM along with chat history. The last string in the list is the prompt.
            guided_decode_json_schema (Optional[str]): JSON schema for guided decoding.
            temperature (float): Controls randomness in the output.
            top_p (float): Controls diversity via nucleus sampling.

        Returns:
            str: The response from the LLM.
        """
        pass

    def query_with_system_prompt(
        self,
        system_prompt: str,
        prompt: str,
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        """
        Abstract method to query an LLM with a given prompt and return the response.

        Args:
            system prompt (str): The system prompt to send to the LLM.
            prompt (str): The prompt to send to the LLM.
            guided_decode_json_schema (Optional[str]): JSON schema for guided decoding.
            temperature (float): Controls randomness in the output.
            top_p (float): Controls diversity via nucleus sampling.

        Returns:
            str: The response from the LLM.
        """
        return self.query(
            system_prompt + "\n" + prompt, guided_decode_json_schema, temperature, top_p
        )

    def _query_with_retries(
        self,
        func: Callable[..., str],
        *args: Union[str, bool, Optional[str], float, float],
        retries: int = NUM_LLM_RETRIES,
        max_retry_timeout: int = MAX_RETRY_TIMEOUT,
        backoff_factor: float = 0.5,
    ) -> str:
        last_exception = None
        for retry in range(retries):
            try:
                return func(*args)
            except Exception as exception:
                last_exception = exception
                sleep_time = (
                    backoff_factor * (2**retry)
                    if backoff_factor * (2**retry) <= max_retry_timeout
                    else max_retry_timeout
                )
                time.sleep(sleep_time)
                LOG.error(
                    f"LLM Query failed with error {type(exception)}: {textwrap.shorten(str(exception), width=250)}. Sleeping for {sleep_time} seconds..."
                )
        raise RetriesExceededException(
            f"Unable to query LLM after {retries} retries: {last_exception}"
        )

    def _chat_with_retries(
        self,
        func: Callable[..., str],
        *args: Union[List[str], bool, Optional[str], float, float],
        retries: int = NUM_LLM_RETRIES,
        max_retry_timeout: int = MAX_RETRY_TIMEOUT,
        backoff_factor: float = 0.5,
    ) -> str:
        last_exception = None
        for retry in range(retries):
            try:
                return func(*args)
            except Exception as exception:
                last_exception = exception
                sleep_time = (
                    backoff_factor * (2**retry)
                    if backoff_factor * (2**retry) <= max_retry_timeout
                    else max_retry_timeout
                )
                time.sleep(sleep_time)
                LOG.error(
                    f"LLM Chat query failed with error: {exception}. Sleeping for {sleep_time} seconds..."
                )
        raise RetriesExceededException(
            f"Unable to query LLM after {retries} retries: {last_exception}"
        )

    def query_with_retries(
        self,
        prompt: str,
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        if self.cache_handler:
            cached_response = self.cache_handler.get_response(self.model, None, prompt)
            if cached_response:
                return cached_response
        response = self._query_with_retries(
            self.query, prompt, guided_decode_json_schema, temperature, top_p
        )

        if self.cache_handler:
            self.cache_handler.save_response(self.model, None, prompt, response)

        return response

    def chat_with_retries(
        self,
        prompt_with_history: List[str],
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        if self.cache_handler:
            cached_response = self.cache_handler.get_response_chat(
                self.model, None, prompt_with_history
            )
            if cached_response:
                return cached_response
        response = self._chat_with_retries(
            self.chat,
            prompt_with_history,
            guided_decode_json_schema,
            temperature,
            top_p,
        )
        if self.cache_handler:
            self.cache_handler.save_response_chat(
                self.model, None, prompt_with_history, response
            )
        return response

    def query_with_system_prompt_with_retries(
        self,
        system_prompt: str,
        prompt: str,
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        if self.cache_handler:
            cached_response = self.cache_handler.get_response(
                self.model, system_prompt, prompt
            )
            if cached_response:
                return cached_response
        response = self._query_with_retries(
            self.query_with_system_prompt,
            system_prompt,
            prompt,
            guided_decode_json_schema,
            temperature,
            top_p,
        )

        if self.cache_handler:
            self.cache_handler.save_response(
                self.model, system_prompt, prompt, response
            )

        return response

    def chat_with_system_prompt_with_retries(
        self,
        system_prompt: str,
        prompt_with_history: List[str],
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        if self.cache_handler:
            cached_response = self.cache_handler.get_response_chat(
                self.model, system_prompt, prompt_with_history
            )
            if cached_response:
                return cached_response
        response = self._chat_with_retries(
            self.chat_with_system_prompt,
            system_prompt,
            prompt_with_history,
            guided_decode_json_schema,
            temperature,
            top_p,
        )
        if self.cache_handler:
            self.cache_handler.save_response_chat(
                self.model, system_prompt, prompt_with_history, response
            )
        return response

    def query_with_system_prompt_and_image(
        self,
        system_prompt: str,
        prompt: str,
        image_path: str,
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        """
        Abstract method to query an LLM with a given prompt, system prompt, and image and return the response.
        """
        LOG.error(
            "[ERROR] Please override this method in a subclass to use this method."
        )
        return "Query API not implemented yet."

    def query_with_system_prompt_and_image_with_retries(
        self,
        system_prompt: str,
        prompt: str,
        image_path: str,
        guided_decode_json_schema: Optional[str] = None,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        return self._query_with_retries(
            self.query_with_system_prompt_and_image,
            system_prompt,
            prompt,
            image_path,
            guided_decode_json_schema,
            temperature,
            top_p,
        )

    def query_multimodal(
        self,
        system_prompt: Optional[str] = None,
        text_prompt: Optional[str] = None,
        image_paths: Optional[List[str]] = None,
        audio_paths: Optional[List[str]] = None,
        max_tokens: int = DEFAULT_MAX_TOKENS,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
    ) -> str:
        """
        Abstract method to query an LLM and return the response. This method
        supports multimodal queries with text, images and audio, and usage of a system prompt.

        Args:
            system_prompt (Optional[str]): The system prompt to send to the LLM.
            text_prompt (Optional[str]): The prompt to send to the LLM.
            image_paths (Optional[List[str]]): The paths to the images to send to the LLM.
            audio_paths (Optional[List[str]]): The paths to the audio files to send to the LLM.
            max_tokens (int): Maximum number of tokens to generate.
            temperature (float): Controls randomness in the output.
            top_p (float): Controls diversity via nucleus sampling.

        Returns:
            Optional[str]: The response from the LLM, or None if it failed to query.
        """
        raise NotImplementedError(
            "Please override this method in a subclass to use this method."
        )

    def query_multimodal_with_retries(
        self,
        system_prompt: Optional[str] = None,
        text_prompt: Optional[str] = None,
        image_paths: Optional[List[str]] = None,
        audio_paths: Optional[List[str]] = None,
        num_retries: int = NUM_LLM_RETRIES,
        max_retry_timeout: int = MAX_RETRY_TIMEOUT,
        max_tokens: int = DEFAULT_MAX_TOKENS,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
        backoff_factor: float = 0.5,
    ) -> Optional[str]:
        """
        Abstract method to query an LLM and return the response, wrapped in retries.
        This method supports multimodal queries and system prompt.

        Args:
            system_prompt (Optional[str]): The system prompt to send to the LLM.
            text_prompt (Optional[str]): The prompt to send to the LLM.
            image_paths (Optional[List[str]]): The paths to the images to send to the LLM.
            audio_paths (Optional[List[str]]): The paths to the audio files to send to the LLM.
            num_retries (int): Number of retries to attempt.
            max_retry_timeout (int): Maximum timeout between retries.
            max_tokens (int): Maximum number of tokens to generate.
            temperature (float): Controls randomness in the output.
            top_p (float): Controls diversity via nucleus sampling.
            backoff_factor (float): Factor to increase backoff time between retries.

        Returns:
            Optional[str]: The response from the LLM, or None if it failed to query.
        """
        import json

        cache_key = json.dumps(
            {
                "text_prompt": text_prompt,
                "image_paths": (
                    sorted(image_paths) if image_paths is not None else None
                ),
                "audio_paths": (
                    sorted(audio_paths) if audio_paths is not None else None
                ),
            }
        )
        if self.cache_handler:
            cached_response = self.cache_handler.get_response(
                self.model, system_prompt, cache_key
            )
            if cached_response:
                return cached_response

        last_exception = None
        for retry in range(num_retries):
            try:
                response = self.query_multimodal(
                    system_prompt=system_prompt,
                    text_prompt=text_prompt,
                    image_paths=image_paths,
                    audio_paths=audio_paths,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                )
                if self.cache_handler:
                    self.cache_handler.save_response(
                        self.model, system_prompt, cache_key, response
                    )
                return response
            except NotImplementedError as exception:
                LOG.error(
                    f"LLM Query failed as the method was not implemented: {exception}"
                )
                raise exception
            except UnretriableQueryException as exception:
                LOG.error(f"LLM Query failed due to unretriable error: {exception}")
                return None
            except Exception as exception:
                last_exception = exception
                sleep_time = (
                    backoff_factor * (2**retry)
                    if backoff_factor * (2**retry) <= max_retry_timeout
                    else max_retry_timeout
                )
                time.sleep(sleep_time)
                LOG.error(
                    f"LLM Query failed with error: {exception}. Sleeping for {sleep_time} seconds..."
                )
        LOG.error(f"Unable to query LLM after {num_retries} retries: {last_exception}")
        return None

    def valid_models(self) -> list[str]:
        """List of valid model parameters, e.g. 'gpt-3.5-turbo' for GPT"""
        return []
